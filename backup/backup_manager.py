#!/usr/bin/env python3
"""
å¤‡ä»½ç®¡ç†å™¨
æ”¯æŒé¡¹ç›®çš„æ™ºèƒ½å¤‡ä»½å’Œæ¢å¤
"""

import os
import json
import time
import shutil
from pathlib import Path
from datetime import datetime

class BackupManager:
    def __init__(self, backup_dir="./backups"):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ–‡ä»¶
        self.config_file = self.backup_dir / "backup_config.json"
        self.load_config()
    
    def load_config(self):
        """åŠ è½½å¤‡ä»½é…ç½®"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
        else:
            self.config = {
                "exclude_patterns": [
                    "*.log", "*.tmp", "__pycache__/", ".git/",
                    "node_modules/", ".vscode/", ".idea/",
                    "*.pyc", "*.pyo", ".DS_Store"
                ],
                "max_backups": 10,
                "compress": True,
                "backup_types": {
                    "full": "å®Œæ•´å¤‡ä»½ - åŒ…å«æ‰€æœ‰æ–‡ä»¶",
                    "code": "ä»£ç å¤‡ä»½ - åªåŒ…å«æºä»£ç ",
                    "quick": "å¿«é€Ÿå¤‡ä»½ - å…³é”®æ–‡ä»¶",
                    "custom": "è‡ªå®šä¹‰å¤‡ä»½ - ç”¨æˆ·é€‰æ‹©"
                }
            }
            self.save_config()
    
    def save_config(self):
        """ä¿å­˜å¤‡ä»½é…ç½®"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
    
    def backup_project(self, project_name, ssh_manager, backup_type="code"):
        """å¤‡ä»½æŒ‡å®šé¡¹ç›®"""
        print(f"ğŸ’¾ å¼€å§‹å¤‡ä»½é¡¹ç›®: {project_name}")
        print(f"ğŸ“¦ å¤‡ä»½ç±»å‹: {backup_type}")
        
        # ç”Ÿæˆå¤‡ä»½åç§°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{project_name}_{backup_type}_{timestamp}"
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        project_backup_dir = self.backup_dir / project_name
        project_backup_dir.mkdir(exist_ok=True)
        
        backup_file = project_backup_dir / f"{backup_name}.tar.gz"
        
        # è·å–é¡¹ç›®è·¯å¾„
        remote_project_path = f"/home/shared/projects/{project_name}"
        
        # æ£€æŸ¥é¡¹ç›®æ˜¯å¦å­˜åœ¨
        if not ssh_manager.file_exists(remote_project_path):
            print(f"âŒ é¡¹ç›®ä¸å­˜åœ¨: {remote_project_path}")
            return False
        
        # æ ¹æ®å¤‡ä»½ç±»å‹åˆ›å»ºæ’é™¤è§„åˆ™
        exclude_rules = self._get_exclude_rules(backup_type)
        
        # åˆ›å»ºå¤‡ä»½å‘½ä»¤
        exclude_options = " ".join([f"--exclude='{pattern}'" for pattern in exclude_rules])
        backup_cmd = f"cd /home/shared/projects && tar -czf /tmp/{backup_name}.tar.gz {exclude_options} {project_name}"
        
        print(f"ğŸ”§ æ‰§è¡Œå¤‡ä»½å‘½ä»¤...")
        stdout, stderr, exit_status = ssh_manager.execute_command(backup_cmd, timeout=1800)  # 30åˆ†é’Ÿè¶…æ—¶
        
        if exit_status != 0:
            print(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {stderr}")
            return False
        
        # ä¸‹è½½å¤‡ä»½æ–‡ä»¶
        print(f"ğŸ“¥ ä¸‹è½½å¤‡ä»½æ–‡ä»¶...")
        remote_backup_path = f"/tmp/{backup_name}.tar.gz"
        
        if not ssh_manager.download_file(remote_backup_path, str(backup_file)):
            print(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸‹è½½å¤±è´¥")
            return False
        
        # æ¸…ç†è¿œç¨‹ä¸´æ—¶æ–‡ä»¶
        ssh_manager.execute_command(f"rm -f {remote_backup_path}")
        
        # åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
        backup_info = {
            "project_name": project_name,
            "backup_type": backup_type,
            "timestamp": timestamp,
            "backup_file": str(backup_file),
            "size": os.path.getsize(backup_file),
            "created_at": datetime.now().isoformat(),
            "exclude_rules": exclude_rules
        }
        
        info_file = project_backup_dir / f"{backup_name}.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(backup_info, f, indent=2, ensure_ascii=False)
        
        # æ¸…ç†æ—§å¤‡ä»½
        self._cleanup_old_backups(project_name)
        
        print(f"âœ… é¡¹ç›®å¤‡ä»½å®Œæˆ: {backup_file}")
        print(f"ğŸ“Š å¤‡ä»½å¤§å°: {self._format_size(backup_info['size'])}")
        
        return True
    
    def _get_exclude_rules(self, backup_type):
        """æ ¹æ®å¤‡ä»½ç±»å‹è·å–æ’é™¤è§„åˆ™"""
        base_excludes = self.config["exclude_patterns"].copy()
        
        if backup_type == "code":
            # ä»£ç å¤‡ä»½ï¼šæ’é™¤æ•°æ®ã€æ¨¡å‹ã€ç»“æœæ–‡ä»¶
            base_excludes.extend([
                "datasets/*", "data/*", "*.dataset",
                "models/*.pth", "models/*.pt", "models/*.ckpt", "*.model",
                "results/*/logs/*", "results/*/checkpoints/*",
                "logs/*", "checkpoints/*", "wandb/*",
                "*.tar.gz", "*.zip", "*.7z"
            ])
        elif backup_type == "quick":
            # å¿«é€Ÿå¤‡ä»½ï¼šåªä¿ç•™å…³é”®ä»£ç å’Œé…ç½®
            base_excludes.extend([
                "datasets/*", "data/*", "models/*", "results/*",
                "logs/*", "checkpoints/*", "wandb/*",
                "*.tar.gz", "*.zip", "*.7z", "*.jpg", "*.png", "*.mp4"
            ])
        elif backup_type == "full":
            # å®Œæ•´å¤‡ä»½ï¼šåªæ’é™¤ä¸´æ—¶æ–‡ä»¶
            pass  # ä½¿ç”¨åŸºç¡€æ’é™¤è§„åˆ™
        
        return base_excludes
    
    def list_backups(self, project_name=None):
        """åˆ—å‡ºå¤‡ä»½"""
        backups = []
        
        if project_name:
            # åˆ—å‡ºæŒ‡å®šé¡¹ç›®çš„å¤‡ä»½
            project_backup_dir = self.backup_dir / project_name
            if project_backup_dir.exists():
                backups.extend(self._scan_project_backups(project_name, project_backup_dir))
        else:
            # åˆ—å‡ºæ‰€æœ‰é¡¹ç›®çš„å¤‡ä»½
            for project_dir in self.backup_dir.iterdir():
                if project_dir.is_dir() and project_dir.name != "." and not project_dir.name.startswith('.'):
                    project_name = project_dir.name
                    backups.extend(self._scan_project_backups(project_name, project_dir))
        
        # æŒ‰æ—¶é—´æ’åº
        backups.sort(key=lambda x: x['created_at'], reverse=True)
        
        if backups:
            print("ğŸ“ å¯ç”¨çš„å¤‡ä»½:")
            print("=" * 80)
            for backup in backups:
                print(f"ğŸ“¦ {backup['project_name']} - {backup['backup_type']}")
                print(f"   ğŸ“… æ—¶é—´: {backup['timestamp']}")
                print(f"   ğŸ“Š å¤§å°: {self._format_size(backup['size'])}")
                print(f"   ğŸ“„ æ–‡ä»¶: {backup['backup_file']}")
                print("-" * 80)
        else:
            print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")
        
        return backups
    
    def _scan_project_backups(self, project_name, project_dir):
        """æ‰«æé¡¹ç›®å¤‡ä»½"""
        backups = []
        
        for info_file in project_dir.glob("*.json"):
            try:
                with open(info_file, 'r', encoding='utf-8') as f:
                    backup_info = json.load(f)
                
                # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                backup_file = Path(backup_info['backup_file'])
                if backup_file.exists():
                    backup_info['size'] = os.path.getsize(backup_file)
                    backups.append(backup_info)
                else:
                    print(f"âš ï¸ å¤‡ä»½æ–‡ä»¶ç¼ºå¤±: {backup_file}")
                    
            except Exception as e:
                print(f"âš ï¸ è¯»å–å¤‡ä»½ä¿¡æ¯å¤±è´¥: {info_file}, {e}")
        
        return backups
    
    def restore_backup(self, backup_file, ssh_manager, restore_path=None):
        """æ¢å¤å¤‡ä»½"""
        backup_file = Path(backup_file)
        if not backup_file.exists():
            print(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
            return False
        
        # è·å–å¤‡ä»½ä¿¡æ¯
        info_file = backup_file.with_suffix('.json')
        if info_file.exists():
            with open(info_file, 'r', encoding='utf-8') as f:
                backup_info = json.load(f)
            project_name = backup_info['project_name']
        else:
            # ä»æ–‡ä»¶åæ¨æ–­é¡¹ç›®å
            parts = backup_file.stem.split('_')
            project_name = parts[0] if parts else "unknown"
        
        if restore_path is None:
            restore_path = f"/home/shared/projects/{project_name}"
        
        print(f"ğŸ”„ æ¢å¤å¤‡ä»½: {backup_file.name}")
        print(f"ğŸ“‚ ç›®æ ‡è·¯å¾„: {restore_path}")
        
        # ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
        remote_backup_path = f"/tmp/restore_{int(time.time())}.tar.gz"
        
        print(f"ğŸ“¤ ä¸Šä¼ å¤‡ä»½æ–‡ä»¶...")
        if not ssh_manager.upload_file(str(backup_file), remote_backup_path):
            print(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
            return False
        
        # åˆ›å»ºæ¢å¤ç›®å½•
        restore_parent = str(Path(restore_path).parent)
        ssh_manager.create_directory(restore_parent)
        
        # è§£å‹å¤‡ä»½
        print(f"ğŸ“¦ è§£å‹å¤‡ä»½æ–‡ä»¶...")
        extract_cmd = f"cd {restore_parent} && tar -xzf {remote_backup_path}"
        stdout, stderr, exit_status = ssh_manager.execute_command(extract_cmd)
        
        if exit_status != 0:
            print(f"âŒ å¤‡ä»½è§£å‹å¤±è´¥: {stderr}")
            ssh_manager.execute_command(f"rm -f {remote_backup_path}")
            return False
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        ssh_manager.execute_command(f"rm -f {remote_backup_path}")
        
        print(f"âœ… å¤‡ä»½æ¢å¤å®Œæˆ: {restore_path}")
        return True
    
    def _cleanup_old_backups(self, project_name):
        """æ¸…ç†æ—§å¤‡ä»½"""
        project_backup_dir = self.backup_dir / project_name
        if not project_backup_dir.exists():
            return
        
        # è·å–æ‰€æœ‰å¤‡ä»½
        backups = self._scan_project_backups(project_name, project_backup_dir)
        
        # å¦‚æœå¤‡ä»½æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œåˆ é™¤æœ€æ—§çš„
        max_backups = self.config.get("max_backups", 10)
        if len(backups) > max_backups:
            # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
            backups.sort(key=lambda x: x['created_at'], reverse=True)
            
            for backup in backups[max_backups:]:
                try:
                    backup_file = Path(backup['backup_file'])
                    info_file = backup_file.with_suffix('.json')
                    
                    if backup_file.exists():
                        backup_file.unlink()
                    if info_file.exists():
                        info_file.unlink()
                    
                    print(f"ğŸ—‘ï¸ æ¸…ç†æ—§å¤‡ä»½: {backup_file.name}")
                    
                except Exception as e:
                    print(f"âš ï¸ æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")
    
    def get_backup_statistics(self):
        """è·å–å¤‡ä»½ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_backups": 0,
            "total_size": 0,
            "projects": {},
            "backup_types": {}
        }
        
        for project_dir in self.backup_dir.iterdir():
            if project_dir.is_dir() and not project_dir.name.startswith('.'):
                project_name = project_dir.name
                project_backups = self._scan_project_backups(project_name, project_dir)
                
                project_stats = {
                    "count": len(project_backups),
                    "total_size": sum(b['size'] for b in project_backups),
                    "latest": max(project_backups, key=lambda x: x['created_at'])['created_at'] if project_backups else None
                }
                
                stats["projects"][project_name] = project_stats
                stats["total_backups"] += project_stats["count"]
                stats["total_size"] += project_stats["total_size"]
                
                # ç»Ÿè®¡å¤‡ä»½ç±»å‹
                for backup in project_backups:
                    backup_type = backup['backup_type']
                    if backup_type not in stats["backup_types"]:
                        stats["backup_types"][backup_type] = 0
                    stats["backup_types"][backup_type] += 1
        
        return stats
    
    def create_backup_schedule(self, project_name, schedule_type="daily", backup_type="code"):
        """åˆ›å»ºå¤‡ä»½è®¡åˆ’"""
        schedule_file = self.backup_dir / "schedules.json"
        
        if schedule_file.exists():
            with open(schedule_file, 'r', encoding='utf-8') as f:
                schedules = json.load(f)
        else:
            schedules = {}
        
        schedules[project_name] = {
            "schedule_type": schedule_type,
            "backup_type": backup_type,
            "enabled": True,
            "last_backup": None,
            "created_at": datetime.now().isoformat()
        }
        
        with open(schedule_file, 'w', encoding='utf-8') as f:
            json.dump(schedules, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å¤‡ä»½è®¡åˆ’å·²åˆ›å»º: {project_name} ({schedule_type})")
        return True
    
    def _format_size(self, size_bytes):
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"
    
    def export_backup_info(self, output_file=None):
        """å¯¼å‡ºå¤‡ä»½ä¿¡æ¯"""
        if output_file is None:
            output_file = self.backup_dir / f"backup_info_{datetime.now().strftime('%Y%m%d')}.json"
        
        stats = self.get_backup_statistics()
        backups = self.list_backups()
        
        export_data = {
            "export_date": datetime.now().isoformat(),
            "statistics": stats,
            "backups": backups,
            "config": self.config
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å¤‡ä»½ä¿¡æ¯å·²å¯¼å‡º: {output_file}")
        return str(output_file) 